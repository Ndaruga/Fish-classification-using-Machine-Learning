{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4514c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.2-cp39-cp39-win_amd64.whl (444.1 MB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Collecting termcolor>=1.1.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/tensorflow-estimator/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=0eec22abb8bc0d8f94aa5237b44d095f807d6ad7dfbeef29f30d6f47cf9ec467\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.6 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "!pip install tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee505d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Fish images\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m val_image_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      8\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m     10\u001b[0m display(labels)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Fish images\\\\train'"
     ]
    }
   ],
   "source": [
    "# Get Fish categories available in the root dir\n",
    "root_dir = \"Fish images\"\n",
    "train_image_dir = os.path.join(root_dir, \"train\")\n",
    "val_image_dir = os.path.join(root_dir, \"val\")\n",
    "\n",
    "labels = []\n",
    "for label in os.listdir(train_image_dir):\n",
    "    labels.append(label)\n",
    "    \n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc6039",
   "metadata": {},
   "source": [
    "### Loading the image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import cv2\n",
    "img_size = 256\n",
    "image_data = []\n",
    "\n",
    "def get_data(image_dir):\n",
    "    for label in labels:\n",
    "        image_path = os.path.join(image_dir, label)\n",
    "        class_num = labels.index(label)  # class num gives each fish category a label\n",
    "        for img in os.listdir(image_path):\n",
    "            img_arr = cv2.imread(os.path.join(image_path, img))\n",
    "    #             resize the images\n",
    "            resized_images = cv2.resize(img_arr, (img_size, img_size))\n",
    "            image_data.append([resized_images, class_num])\n",
    "    return np.array(image_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm_notebook as tqdm_nb\n",
    "# import time\n",
    "\n",
    "train = get_data(train_image_dir)\n",
    "val = get_data(val_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22f2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32983259",
   "metadata": {},
   "source": [
    "#### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "\n",
    "x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range = 30,\n",
    "        zoom_range = 0.2, \n",
    "        width_shift_range=0.1,  \n",
    "        height_shift_range=0.1, \n",
    "        horizontal_flip = True, \n",
    "        vertical_flip=False)\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d465a76",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd17484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add first convo layer, maxpool and dropout\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5,5), \n",
    "                 activation = 'relu', input_shape = (img_size, img_size,3)))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add a second convolution layer, maxpoling and dropout\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add a third convolution layer, maxpoling and dropout\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Add a flattening layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# add a layer with 5000 neurons and dropout\n",
    "model.add(Dense(5000, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add a layer with 1000 neurons\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add a layer with 500 neurons\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add a layer with 250 neurons\n",
    "model.add(Dense(250, activation = 'relu'))\n",
    "\n",
    "# add a layer with 10 neurons\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df481c07",
   "metadata": {},
   "source": [
    "#### Model compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac41d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(learning_rate=0.000001)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7b615",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d684c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement callback function to stop training  when accuracy reaches 96%\n",
    "ACCURACY_THRESHOLD = 0.96\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\tif(logs.get('accuracy') > ACCURACY_THRESHOLD):\n",
    "\t\t\tprint(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))\n",
    "\t\t\tself.model.stop_training = True\n",
    "\n",
    "# Instantiate a callback object\n",
    "callbacks = myCallback()\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 shuffle = True,\n",
    "                 batch_size = 150,\n",
    "                 epochs = 500,\n",
    "                 validation_data = (x_val, y_val),\n",
    "                 callbacks=[callbacks]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b2ba9",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = model.evaluate(x_val, y_val)[1]*100\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db68ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Val'], loc ='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Val'], loc ='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27baade9",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "# !pip install scikit-image\n",
    "from skimage.transform import resize\n",
    "\n",
    "foldername = \"./test_images/\" ###### folder for test images\n",
    "\n",
    "for img_count, filename in enumerate(os.listdir(foldername)):\n",
    "    img = plt.imread(os.path.join(foldername,filename))\n",
    "    resized_image = resize(img, (img_size, img_size, 3))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "      \n",
    "    predictions = model.predict(np.array([resized_image]))\n",
    "    print(predictions)\n",
    "    list_index = [0,1,2,3,4,5,6,7,8,9]\n",
    "    x = predictions\n",
    "\n",
    "  ################ display predictions as index from 0-9. ##############\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if x[0][list_index[i]] > x[0][list_index[j]]:\n",
    "                #swap the numbers\n",
    "                temp = list_index[i]\n",
    "                list_index[i] = list_index[j]\n",
    "                list_index[j] = temp\n",
    "    display(list_index)\n",
    "    print(' '*50)\n",
    "    for i in range(len(labels)):\n",
    "        print(labels[list_index[i]], ':', round((predictions[0][list_index[i]] * 100),2), '%')\n",
    "    print(\"*\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71001206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
